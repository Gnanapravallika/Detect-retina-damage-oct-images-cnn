{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"hide_input":false,"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":23942,"sourceType":"datasetVersion","datasetId":17839}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nfrom glob import glob\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing import image\nfrom matplotlib.image import imread\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import BatchNormalization\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom tensorflow.keras.models import Sequential, Model\nfrom keras.regularizers import l2\nfrom tensorflow.keras.layers import Activation, Dropout, Dense, Flatten, Conv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D,Input,concatenate, Lambda\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"_7n6WaIiVsK3","jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Detect Retina Damage From OCT Images With CNN","metadata":{}},{"cell_type":"code","source":"import os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"Retinal optical coherence tomography (OCT) is an imaging technique used to capture high-resolution cross sections of the retinas of living patients. Approximately 30 million OCT scans are performed each year, and the analysis and interpretation of these images take up a significant amount of time (Swanson and Fujimoto, 2017).\n\nFigure 2. Representative Optical Coherence Tomography Images and the Workflow Diagram [Kermany et al., 2018] http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5\n\n(A) (Far left) Choroidal neovascularization (CNV) with neovascular membrane (white arrowheads) and associated subretinal fluid (arrows). (Middle left) Diabetic macular edema (DME) with retinal-thickening-associated intraretinal fluid (arrows). (Middle right) Multiple drusen (arrowheads) present in early AMD. (Far right) Normal retina with preserved foveal contour and absence of any retinal fluid/edema.\n\nThe dataset is organized into 3 folders (train, test, val) and contains subfolders for each image category (NORMAL, CNV, DME, DRUSEN). There are 84,495 X-Ray images (JPEG) and 4 categories (NORMAL, CNV, DME, DRUSEN).\n\nImages are labeled as (disease)-(randomized patient ID)-(image number by this patient) and split into 4 directories: CNV, DME, DRUSEN, and NORMAL.\n\nCNV (Choroidal Neovascularization): Choroidal neovascularization refers to the abnormal growth of new blood vessels beneath the retina. This condition is typically associated with diseases like age-related macular degeneration.\n\nDME (Diabetic Macular Edema): Diabetic macular edema is the accumulation of fluid in the macula region of the retina due to diabetic retinopathy. It can lead to vision loss.\n\nDRUSEN: Drusen are small yellowish or whitish deposits seen on the retina. They are considered early signs of diseases like age-related macular degeneration (AMD).\n\nNORMAL: Normal retina images represent healthy retinal cross-sections without any pathological signs or abnormalities.\n\nThese terms play a significant role in the diagnosis and monitoring of retinal diseases in medical studies utilizing optical coherence tomography (OCT) imaging technology.\n","metadata":{}},{"cell_type":"code","source":"# Main Folder Path\nfolder_path = \"/kaggle/input/kermany2018/OCT2017 \"\n\n# Sub Folder Paths\ntrain_dir = f\"{folder_path}/train\"\nval_dir = f\"{folder_path}/val\"\ntest_dir = f\"{folder_path}/test\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.listdir(folder_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Train Directory: {os.listdir(train_dir)}\")\nprint(f\"Validation Directory: {os.listdir(test_dir)}\")\nprint(f\"Test Directory: {os.listdir(val_dir)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normal_train_dir = os.path.join(train_dir, \"NORMAL\")\nnormal_train_files = os.listdir(normal_train_dir)[:30]\n\nnormal_train_files","metadata":{"id":"p68mTCUIYMDk","outputId":"663cc2c2-4ee8-4b11-cbbd-585f61bdf0ed","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"normal_train_files[17]","metadata":{"id":"OKiKiJfHYMKs","outputId":"f97df862-4d6c-4da7-ab76-4d2de7e906a6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Image file path\nimage_file = \"NORMAL/NORMAL-8869683-18.jpeg\"\nimage_path = os.path.join(train_dir, image_file)\n\n# Read and display the image\nimage = Image.open(image_path)\nplt.imshow(image)\nplt.axis('off')  # Hide axes\nplt.show()","metadata":{"id":"yUkx4AiqYkj_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Specify the directory where the dataset is located\ndataset_directory = train_dir\n\n# Create a dictionary to store the counts of images for each class\nimage_counts = {\"CNV\": 0, \"DME\": 0, \"DRUSEN\": 0, \"NORMAL\": 0}\n\n# Iterate through the dataset to count the number of images for each class\nfor class_name in image_counts.keys():\n    class_directory = os.path.join(dataset_directory, class_name)\n    image_counts[class_name] = len(os.listdir(class_directory))\n\n# Plotting the graph\nclasses = list(image_counts.keys())\ncounts = list(image_counts.values())\n\nfig, ax = plt.subplots()\nbars = ax.bar(classes, counts, color=['#FF9999', '#66B2FF', '#99FF99', '#FFCC99'])\n\n# Display total counts above the bars\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.1, round(yval), ha='center', va='bottom')\n\nplt.xlabel('Classes')\nplt.ylabel('Number of Images')\nplt.title('Number of Images in Train Classes')\nplt.show()","metadata":{"id":"t0O8h2OtaWMx","outputId":"e1c2da66-4873-46cb-ded0-ec84c7c1185b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Specify the directory where the dataset is located\ndataset_directory = test_dir\n\n# Create a dictionary to store the counts of images for each class\nimage_counts = {\"CNV\": 0, \"DME\": 0, \"DRUSEN\": 0, \"NORMAL\": 0}\n\n# Iterate through the dataset to count the number of images for each class\nfor class_name in image_counts.keys():\n    class_directory = os.path.join(dataset_directory, class_name)\n    image_counts[class_name] = len(os.listdir(class_directory))\n\n# Plotting the graph\nclasses = list(image_counts.keys())\ncounts = list(image_counts.values())\n\nfig, ax = plt.subplots()\nbars = ax.bar(classes, counts, color=['#FF9999', '#66B2FF', '#99FF99', '#FFCC99'])\n\n# Display total counts above the bars\nfor bar in bars:\n    yval = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2, yval + 0.1, round(yval), ha='center', va='bottom')\n\nplt.xlabel('Classes')\nplt.ylabel('Number of Images')\nplt.title('Number of Images in Test Classes')\nplt.show()","metadata":{"id":"n7EikWR-Lz39","outputId":"ab545f3c-e744-4473-ba9f-66f4e90374aa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Specify the directory where the dataset is located\ndataset_directory = train_dir\n\n# Specify the classes\nclasses = [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]\n\n# Loop to return the shape of an image from each class\nfor class_name in classes:\n    # Create the class directory\n    class_directory = os.path.join(dataset_directory, class_name)\n\n    # Select an image in the class directory\n    image = os.listdir(class_directory)[55]\n\n    # Load the image\n    image_path = os.path.join(class_directory, image)\n    image = cv2.imread(image_path)\n\n    # Print the shape of the image\n    print(f\"Class: {class_name}, Image Shape: {image.shape}\")","metadata":{"id":"U_L5UcUDL9gq","outputId":"7869ec83-5f99-4a0f-e6f9-8090f5f1ae2f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Specify the directory where the dataset is located\ndataset_directory = train_dir\n\n# Specify the classes\nclasses = [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]\n\n# Loop to return the shape of an image from each class\nfor class_name in classes:\n    # Create the class directory\n    class_directory = os.path.join(dataset_directory, class_name)\n\n    # Select an image in the class directory\n    image = os.listdir(class_directory)[55]\n\n    # Load the image\n    image_path = os.path.join(class_directory, image)\n    image = cv2.imread(image_path)\n\n    # Print the shape of the image\n    print(f\"Class: {class_name}, Image Shape: {image.shape}\")\n\n    x= []\n    y = []\n\n    img = image\n    d1,d2,colors = img.shape\n    x.append(d1)\n    y.append(d2)\n\nprint(np.mean(x))\nprint(np.mean(y))\n\n","metadata":{"id":"Y1rfBcx8W5GK","outputId":"990c4090-dc55-4b72-f157-0dabb0298bd0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Specify the classes\nclasses = [\"CNV\", \"DME\", \"DRUSEN\", \"NORMAL\"]\n\n# Create a Matplotlib figure and axes\nfig, axs = plt.subplots(1, 4, figsize=(12, 3))\n\n# Load and display one example from each class side by side\nfor i, class_name in enumerate(classes):\n    image_path = os.path.join(train_dir, class_name, os.listdir(os.path.join(train_dir, class_name))[0])\n    img = imread(image_path)\n    axs[i].imshow(img)\n    axs[i].axis('off')\n    axs[i].set_title(f\" {class_name}\")\n\nplt.show()","metadata":{"id":"WEUJh1I6Oprz","outputId":"18f15dff-4436-4b4a-8701-f78c823db054","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model 1","metadata":{}},{"cell_type":"markdown","source":"## Creating the Model","metadata":{}},{"cell_type":"code","source":"batch_size = 32","metadata":{"id":"E6o73IATRESP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_shape = (299,299,1)","metadata":{"id":"ZkE-Aza42nkT","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.20, # Shift the pic width by a max of 20%\n                               height_shift_range=0.15, # Shift the pic height by a max of 15%\n                               rescale=1/255, # Rescale the image by normalzing it.\n                               shear_range=0.15, # Shear means cutting away part of the image (max 15%)\n                               zoom_range=0.2, # Zoom in by 20% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n                              )","metadata":{"id":"9NrA0UM70bcb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_gen.flow_from_directory(train_dir)","metadata":{"id":"BY2K8rH00b3e","outputId":"646deb3f-37f7-4b61-fdf5-610655a06888","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_gen.flow_from_directory(test_dir)","metadata":{"id":"qfvt6akK0cBW","outputId":"b7afde93-e549-4e68-f18d-7c5d2518398d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=image_shape, padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Dense(4, activation='softmax'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\",\n              optimizer='adam',\n              metrics=['accuracy'])","metadata":{"id":"ZS6ARSpC57xP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"id":"94FMORDf58yI","outputId":"8cf02c43-f66b-4e35-c425-a90a078e3742","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping","metadata":{"id":"2aDsqe4y7eRG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss',patience=5)","metadata":{"id":"Xu5_qt4H7nlD","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_image_gen = image_gen.flow_from_directory(train_dir,\n                                               target_size=(299,299),\n                                               color_mode='grayscale',\n                                               batch_size=batch_size,\n                                               class_mode='categorical', shuffle=True)","metadata":{"id":"53ku4Bvz7ylN","outputId":"4b42e312-3068-445f-fec9-b992e51c6840","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image_gen = image_gen.flow_from_directory(test_dir,\n                                               target_size=(299,299),\n                                               color_mode='grayscale',\n                                               batch_size=batch_size,\n                                               class_mode='categorical', shuffle=False)","metadata":{"id":"jNRjkoRm9HAw","outputId":"c75d5c5b-7cee-4048-85ae-1a724d0ea5b1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = model.fit(train_image_gen,epochs=10, validation_data=test_image_gen, callbacks=[early_stop])","metadata":{"id":"7VVCEpHU9HLE","outputId":"1f700575-61e1-4f70-e715-8fab773c416e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary = pd.DataFrame(model.history.history)\nsummary.head()","metadata":{"id":"wz3FclDF9HTj","outputId":"70aea6ba-d397-4af5-bd55-23a0a83a2cd1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.history.history","metadata":{"id":"f_YffnGAHUWY","outputId":"e70d8075-7ee7-41e0-ab3a-99cf15717ef3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history_df = pd.DataFrame(model.history.history)\nhistory_df.index.name = 'Epoch'\nhistory_df.index += 1  # Epoch numaralarını 1'den başlat\n\n# DataFrame'i yazdır\nprint(history_df)","metadata":{"id":"jnX8MwwnIqyE","outputId":"f48308ad-3c1b-4a1f-ec3e-85f2ea50c230","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(summary.loss, label=\"loss\")\nplt.plot(summary.val_loss, label=\"val_loss\")\nplt.legend(loc=\"upper right\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"8qhZKJFv9Hdz","outputId":"22749a96-7618-4774-823e-d5685b9daabe","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(summary.accuracy, label=\"accuracy\")\nplt.plot(summary.val_accuracy, label=\"val_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"A0lQ79hV9Hue","outputId":"f7c1b746-e9ae-461e-c647-12756f15fa4e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.evaluate(test_image_gen)","metadata":{"id":"67b9fVSZJzxP","outputId":"8a0c5156-8eb9-4635-9412-d888bd766dc7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.metrics_names","metadata":{"id":"mJTLSwYqJz7m","outputId":"a30907c1-879e-46f8-ec47-320011e13287","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_probabilities = model.predict(test_image_gen)","metadata":{"id":"I3Lnixj7KJo9","outputId":"81cd02ce-f8ce-422e-8569-8921386794f3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_probabilities","metadata":{"id":"YMdMBSotKoCN","outputId":"2a879580-a112-4089-9141-c6d0dc2d5b45","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image_gen.classes","metadata":{"id":"4hhfy0prKoL2","outputId":"852c1a06-c13e-49a3-b729-cf5939e527bc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = pred_probabilities","metadata":{"id":"piEUC1V-KoVC","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_argmax = np.argmax(predictions, axis=1)\n\nprint(classification_report(test_image_gen.classes, predictions_argmax))","metadata":{"id":"2AIUMWhUKJ9_","outputId":"dbfc2b33-3d27-4b3f-f541-e5b57bf0670b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('CNN_model1.h5')","metadata":{"id":"ACN49T9dNksz","outputId":"70c3f799-042d-4bc7-e00b-fb63053e923e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# INCEPTİONV3","metadata":{"id":"KdeRjGeKNliU"}},{"cell_type":"code","source":"image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.20, # Shift the pic width by a max of 20%\n                               height_shift_range=0.15, # Shift the pic height by a max of 15%\n                               rescale=1/255, # Rescale the image by normalzing it.\n                               shear_range=0.15, # Shear means cutting away part of the image (max 15%)\n                               zoom_range=0.2, # Zoom in by 20% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n                              )\nbatch_size = 32","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_image_gen = image_gen.flow_from_directory(train_dir,\n                                               target_size=(299,299),\n                                               color_mode='grayscale',\n                                               batch_size=batch_size,\n                                               class_mode='categorical', shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image_gen = image_gen.flow_from_directory(test_dir,\n                                               target_size=(299,299),\n                                        \n                                               color_mode='grayscale',\n                                               batch_size=batch_size,\n                                               class_mode='categorical', shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define input shape\ninput_shape = (299, 299, 1)  # Grayscale images with shape 299x299\n\n# Create the input layer\ninputs = Input(shape=input_shape)\n\n# Convert grayscale to RGB by repeating the single channel across three channels\nx = tf.keras.layers.Concatenate()([inputs, inputs, inputs])\n\n# Load the InceptionV3 model with ImageNet weights, without the top layers, and set the input tensor\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=x)\n\n# Flatten the output of the base model\nx = Flatten()(base_model.output)\n\n# Fully connected layers\nx = Dense(512, activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\nx = Dense(256, activation='relu')(x)\nx = BatchNormalization()(x)\nx = Dropout(0.3)(x)\n\nx = Dense(64, activation='relu')(x)\n\n# Output layer\noutputs = Dense(4, activation='softmax')(x)\n\n# Create the model\nInceptionV3_model = Model(inputs=inputs, outputs=outputs)\n\n# Optimizer\noptimizer = Adam(learning_rate=0.0001)\n\n# Compile the model\nInceptionV3_model.compile(loss='categorical_crossentropy',\n                          optimizer=optimizer,\n                          metrics=['accuracy'])\n\n# Print the model summary\nInceptionV3_model.summary()\n\n","metadata":{"id":"VW6XF3WjNAE4","outputId":"912ea5c1-a998-462d-fe53-f04e98831d2f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"early_stop = EarlyStopping(monitor='val_loss', patience = 4)\ncheckpoint = ModelCheckpoint('InceptionV3_tuning.keras',\n                             monitor='val_loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='auto')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"InceptionV3_model_results = InceptionV3_model.fit(train_image_gen,epochs=10,\n                    validation_data=test_image_gen,\n                    callbacks=[early_stop, checkpoint])","metadata":{"id":"j1oeNQFbNAL4","outputId":"dee18df5-26ab-46b8-fcac-a4b6311fc1b5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary = pd.DataFrame(InceptionV3_model.history.history)\nsummary","metadata":{"id":"DvOgFN8kjG34","outputId":"7cf972e5-e6c0-4efe-8db5-b2b9b6b0ce90","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(summary.loss, label=\"loss\")\nplt.plot(summary.val_loss, label=\"val_loss\")\nplt.legend(loc=\"upper right\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"-XhUMbJgNAgD","outputId":"94e23c04-9e6f-4119-d125-4152bbdc4312","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(summary.accuracy, label=\"accuracy\")\nplt.plot(summary.val_accuracy, label=\"val_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"OkebldLPNAmQ","outputId":"60f3f1e5-6c5d-498d-d519-9219dec8723f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_probabilities = InceptionV3_model.predict(test_image_gen)","metadata":{"id":"Qp8w0htSNAtl","outputId":"8af27335-4bf6-4b36-f55f-2101899f23db","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_argmax = np.argmax(pred_probabilities, axis=1)\n\nprint(classification_report(test_image_gen.classes, predictions_argmax))","metadata":{"id":"FtAt_AyFkNGu","outputId":"d28ba634-ca27-4abe-d176-2ae2610a32c8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matrix = confusion_matrix(test_image_gen.classes, predictions_argmax)\nprint(conf_matrix)\n\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# InceptionV3 Fine-Tuning Model","metadata":{}},{"cell_type":"code","source":"image_gen = ImageDataGenerator(rotation_range=20, # rotate the image 20 degrees\n                               width_shift_range=0.20, # Shift the pic width by a max of 20%\n                               height_shift_range=0.15, # Shift the pic height by a max of 15%\n                               rescale=1/255, # Rescale the image by normalzing it.\n                               shear_range=0.15, # Shear means cutting away part of the image (max 15%)\n                               zoom_range=0.2, # Zoom in by 20% max\n                               horizontal_flip=True, # Allo horizontal flipping\n                               fill_mode='nearest' # Fill in missing pixels with the nearest filled value\n                              )\nbatch_size = 32","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_image_gen = image_gen.flow_from_directory(train_dir,\n                                               target_size=(299,299),\n                                               color_mode='grayscale',\n                                               batch_size=batch_size,\n                                               class_mode='categorical', shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_image_gen = image_gen.flow_from_directory(test_dir,\n                                               target_size=(299,299),\n                                               color_mode='grayscale',\n                                               batch_size=batch_size,\n                                               class_mode='categorical', shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define input shape\ninput_shape = (299, 299, 1)  # Grayscale images with shape 299x299\n\n# Create the input layer\ninputs = Input(shape=input_shape)\n\n# Convert grayscale to RGB by repeating the single channel across three channels\nx = tf.keras.layers.Concatenate()([inputs, inputs, inputs])\n\n# Load the InceptionV3 model with ImageNet weights, without the top layers, and set the input tensor\nbase_model = InceptionV3(weights='imagenet', include_top=False, input_tensor=x)\n\n# Flatten the output of the base model\nx = Flatten()(base_model.output)\n\n# Fully connected layers with L2 regularization\nx = Dense(512, activation='relu', kernel_regularizer=l2(0.01))(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)  # Increased dropout rate\n\nx = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(x)\nx = BatchNormalization()(x)\nx = Dropout(0.5)(x)  # Increased dropout rate\n\nx = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(x)\n\n# Output layer\noutputs = Dense(4, activation='softmax')(x)\n\n# Create the model\nInceptionV3_model = Model(inputs=inputs, outputs=outputs)\n\n# Optimizer\noptimizer = Adam(learning_rate=1e-5)\n\n# Early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7)\n\ncheckpoint = ModelCheckpoint('InceptionV3_tuning.keras',\n                             monitor='val_loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='auto')\n\n\n# Compile the model\nInceptionV3_model.compile(loss='categorical_crossentropy',\n                          optimizer=optimizer,\n                          metrics=['accuracy'])\n\n# Print the model summary\nInceptionV3_model.summary()\n\n# Data augmentation\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n","metadata":{"id":"ue8gEObP6rwI","outputId":"e9de0a31-4c2b-448d-8e99-75b26326f324","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"InceptionV3_model_results = InceptionV3_model.fit(train_image_gen,epochs=20,\n                    validation_data=test_image_gen,\n                    callbacks=[early_stopping, checkpoint])","metadata":{"id":"XBiTAr0k7pWn","outputId":"ab16e3bf-5053-4aaa-d44b-fb542451689d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary = pd.DataFrame(InceptionV3_model.history.history)\nsummary","metadata":{"id":"FXSKaAZ0cq1D","outputId":"588e5390-4bd2-4e7f-9f43-0ab86acb4418","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(summary.loss, label=\"loss\")\nplt.plot(summary.val_loss, label=\"val_loss\")\nplt.legend(loc=\"upper right\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"c1nqcwssL4US","outputId":"d8757114-40f8-4df1-e252-b59f22bcd395","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(summary.accuracy, label=\"accuracy\")\nplt.plot(summary.val_accuracy, label=\"val_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.show()","metadata":{"id":"360lkSYdL4uN","outputId":"ba892779-2a5a-4989-c82e-306827c2bfff","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred_probabilities = InceptionV3_model.predict(test_image_gen)","metadata":{"id":"eA7jymcML48o","outputId":"6a758086-6000-4721-b8f4-9bc44bda436b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions_argmax = np.argmax(pred_probabilities, axis=1)\n\nprint(classification_report(test_image_gen.classes, predictions_argmax))","metadata":{"id":"q1JmDDAxL5Id","outputId":"2b8e0e14-a018-4b48-af0d-8d1e0eb26ad7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conf_matrix = confusion_matrix(test_image_gen.classes, predictions_argmax)\nprint(conf_matrix)\n\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"id":"cfl1MWIhL5r9","outputId":"538b68f2-916d-417f-8bf9-75682558942e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  Evaluating Model Performance on Unseen Validation Data\n","metadata":{}},{"cell_type":"code","source":"val_datagen = ImageDataGenerator(rescale=1./255)\nval_gen = image_gen.flow_from_directory(val_dir,\n                                               target_size=(299,299),\n                                               color_mode='grayscale',\n                                               batch_size=batch_size,\n                                               class_mode='categorical', shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_gen.reset()\nval_probabilities = InceptionV3_model.predict(val_gen)\nval_predictions_argmax = np.argmax(val_probabilities, axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(classification_report(val_gen.classes, val_predictions_argmax))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}